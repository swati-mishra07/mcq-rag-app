{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive RAG MCQ Generator\n",
        "\n",
        "This notebook contains:\n",
        "\n",
        "- AI2-ARC dataset processing\n",
        "- LoRA fine-tuning of Flan-T5\n",
        "- FAISS index building\n",
        "- Model saving for deployment\n"
      ],
      "metadata": {
        "id": "HKJDOS1iUN2J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB1bNUtZuCEg"
      },
      "source": [
        "# INSTALL DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJu6d4ckptp5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate sentence-transformers faiss-cpu evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKQaDa4wNnVW"
      },
      "source": [
        "# LOAD ARC DATASET(Hugging face)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "easy = load_dataset(\"ai2_arc\", \"ARC-Easy\")\n",
        "hard = load_dataset(\"ai2_arc\", \"ARC-Challenge\") # Changed 'ARC-Hard' to 'ARC-Challenge'\n",
        "\n",
        "df_easy = pd.DataFrame(easy[\"train\"])\n",
        "df_hard = pd.DataFrame(hard[\"train\"])\n",
        "\n",
        "df = pd.concat([df_easy, df_hard], ignore_index=True)\n",
        "df = df.dropna(subset=[\"question\"])\n",
        "\n",
        "df = df[[\"question\", \"choices\", \"answerKey\"]]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_mbcRlPgfSNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rFVKVqLznUH"
      },
      "source": [
        "# CONVERT TO PROMPT FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_mcq(example):\n",
        "    choices = example[\"choices\"][\"text\"]\n",
        "    labels = example[\"choices\"][\"label\"]\n",
        "\n",
        "    options = \"\\n\".join([f\"{l}. {c}\" for l, c in zip(labels, choices)])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Create a multiple choice question.\n",
        "\n",
        "Question: {example['question']}\n",
        "\n",
        "Options:\n",
        "{options}\n",
        "\n",
        "Answer: {example['answerKey']}\n",
        "\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(format_mcq)\n",
        "\n"
      ],
      "metadata": {
        "id": "zwDqPyRFfMSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba-vQIQzz3LJ"
      },
      "source": [
        "# LOAD MODEL + LoRA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "4QBn4NVJfCTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo4-_e2v0F7v"
      },
      "source": [
        "# TOKENIZE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    # Tokenize the input text (which is a list of strings when batched=True)\n",
        "    tokenized_inputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Process each text item in the batch to extract and tokenize its answer\n",
        "    tokenized_labels_batch = []\n",
        "    for text_item in example[\"text\"]:\n",
        "        answer_start = text_item.rfind(\"Answer: \") + len(\"Answer: \")\n",
        "        answer = text_item[answer_start:].strip()\n",
        "\n",
        "        tokenized_labels = tokenizer(\n",
        "            answer,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512\n",
        "        ).input_ids\n",
        "        tokenized_labels_batch.append(tokenized_labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = tokenized_labels_batch\n",
        "    return tokenized_inputs\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "onuBX_xzfAFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWGhgMm70pcP"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mcq-model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "PLyl0Cele9Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25nkVu8p1Ko7"
      },
      "source": [
        "# BUILD FAISS INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBjwsJcK1QFA"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "questions = df[\"question\"].tolist()\n",
        "embeddings = embedder.encode(questions, convert_to_numpy=True)\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "faiss.write_index(index, \"arc_faiss.index\")\n",
        "df.to_csv(\"arc_data.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9du14T51E-e"
      },
      "source": [
        "# SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtDXO1411HYt"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"mcq_lora_model\")\n",
        "tokenizer.save_pretrained(\"mcq_lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"mcq_lora_model\", 'zip', \"mcq_lora_model\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"mcq_lora_model.zip\")\n"
      ],
      "metadata": {
        "id": "HisxbxovOK_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}