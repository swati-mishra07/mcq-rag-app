---
title: Adaptive RAG MCQ Generator
emoji: ğŸ§ 
colorFrom: indigo
colorTo: blue
sdk: streamlit
sdk_version: "1.32.2"
app_file: app.py
pinned: false
---

# ğŸ§  Adaptive LoRA Fine-Tuned MCQ Generator

A Streamlit-based NLP application that compares a base sequence-to-sequence model with a LoRA fine-tuned variant for automatic MCQ generation.

The system evaluates model outputs using BLEU, ROUGE, and BERTScore metrics and benchmarks inference time.


## ğŸš€ Features

ğŸ¤– Base Model: google/flan-t5-base

ğŸ¯ LoRA Fine-Tuning using PEFT

ğŸ“Š Automatic Evaluation (BLEU, ROUGE-L, BERTScore)

âš¡ Inference Time Comparison

ğŸŒ Interactive Streamlit Web Interface

---

## ğŸ—ï¸ Project Structure
mcq-rag-app/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ mcq_lora_model/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.bin / .safetensors
â””â”€â”€ README.md

---

## ğŸ“Š Evaluation Metrics

The system compares generated MCQs using:

BLEU

ROUGE-L

BERTScore (semantic similarity)

Metrics are computed against a user-provided reference answer.


---

## ğŸ§ª Model Comparison

The app compares:

Base FLAN-T5 Model

LoRA Fine-Tuned Model

Across:

Output quality

Inference time

Automatic evaluation metrics

---

## ğŸ–¥ï¸ Run Locally

```bash
pip install -r requirements.txt
streamlit run app.py

---

## ğŸ“ˆ Real-World Applications

Educational content generation

Automated assessment systems

EdTech platforms

AI-assisted curriculum design

  
---

## ğŸ”® Future Improvements

Implement real Retrieval-Augmented Generation (FAISS-based)

Batch dataset evaluation

Visualization dashboards

REST API deployment

Model merging for optimized inference

---

## ğŸ‘©â€ğŸ’» Author

**Swati Mishra**  
GitHub: https://github.com/swati-mishra07  

---
