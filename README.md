# ğŸ§  Adaptive LoRA Fined-Tuned MCQ Generator

Built a LoRA fine-tuned MCQ generation system comparing base and adapted models using evaluation metrics (BLEU, ROUGE, BERTScore) with Streamlit deployment.

- ğŸ” FAISS-based semantic retrieval
- ğŸ¤– FLAN-T5 Base
- ğŸ¯ LoRA Fine-Tuning (PEFT)
- ğŸ“Š Automatic Evaluation (ROUGE, BLEU, BERTScore)
- âš¡ Inference Time Benchmarking
- ğŸŒ Streamlit Web Interface

---

## ğŸš€ Features

- RAG-based context retrieval
- Base vs LoRA model comparison
- Inference time logging
- Automatic evaluation metrics
- Modular architecture
- Research-style evaluation pipeline

---

## ğŸ—ï¸ Project Structure
mcq-rag-app/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ model_loader.py
â”‚ â”œâ”€â”€ rag_retriever.py
â”‚ â”œâ”€â”€ prompt_builder.py
â”‚ â””â”€â”€ evaluation.py
â”‚
â””â”€â”€ data/


---

## ğŸ“Š Evaluation Metrics

- ROUGE-L
- BLEU
- BERTScore (Semantic Similarity)

---

## ğŸ§ª Model Comparison

The system compares:

- Base FLAN-T5
- LoRA Fine-Tuned Model

Including:
- Output quality
- Inference time
- Metric scores

---

## ğŸ–¥ï¸ Run Locally

```bash
pip install -r requirements.txt
streamlit run app.py

---

## ğŸ“Š Why LoRA?

LoRA enables parameter-efficient fine-tuning by updating only a small subset of model parameters instead of the entire network.

### âœ… Key Benefits

- Reduced memory usage  
- Faster training  
- Lower deployment cost  
- Efficient domain adaptation  

---

## ğŸ“ˆ Real-World Applications

- EdTech platforms  
- Automated assessment systems  
- Exam preparation tools  
- AI-driven content generation  
- Learning management systems  

---

## ğŸ”® Future Improvements

- Deploy to HuggingFace Spaces  
- Add evaluation metrics (BLEU / ROUGE)  
- Build REST API endpoint  
- Implement authentication system  
- Optimize inference via model merging  
- Add logging & monitoring  

---

## ğŸ‘©â€ğŸ’» Author

**Swati Mishra**  
GitHub: https://github.com/swati-mishra07  

---
